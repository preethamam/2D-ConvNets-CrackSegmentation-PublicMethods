{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dims_width = 448\n",
    "img_dims_height = 448\n",
    "\n",
    "class Dataset(BaseDataset):\n",
    "\n",
    "    CLASSES = ['non-crack', 'crack']\n",
    "    def __init__(self, images_dir, masks_dir, classes=['crack'], transform=None, n_classes=2):\n",
    "        self.train_ids = natsorted(next(os.walk(images_dir))[2])\n",
    "        self.mask_ids = natsorted(next(os.walk(masks_dir))[2])\n",
    "\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.train_ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, mask_id) for mask_id in self.mask_ids]\n",
    "\n",
    "        self.class_values= [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_rows = img_dims_width\n",
    "        self.img_cols = img_dims_height\n",
    "        self.img_width = img_dims_width\n",
    "        self.img_height = img_dims_height\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        mask = cv2.imread(self.masks_fps[i],0)\n",
    "\n",
    "        image = cv2.resize(image, dsize=(self.img_width, self.img_height))\n",
    "        mask = cv2.resize(mask, dsize=(self.img_width, self.img_height))\n",
    "        if self.transform:\n",
    "            img, mask = self.transform(image), self.transform(mask)\n",
    "        masks = [(mask==v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        image = torch.from_numpy(image).permute(2,0,1).float()\n",
    "        mask = torch.from_numpy(mask).permute(2,0,1).float()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(3, 64,kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding = 1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding = 1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding = 1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1_1 = nn.Conv2d(256, 64, kernel_size=1, padding=1)\n",
    "        self.conv3_2_1 = nn.Conv2d(256, 64, kernel_size=1, padding=1)\n",
    "        self.conv3_3_1 = nn.Conv2d(256, 64, kernel_size=1, padding=1)\n",
    "        self.conv3_1_3 = nn.Conv2d(64, 12, kernel_size = 1, padding = 1)\n",
    "        self.conv3_1_4 = nn.Conv2d(12, 1, kernel_size = 1, padding=0)\n",
    "        self.conv3_1_5 = nn.Conv2d(1, 1, kernel_size=1, padding=0)\n",
    "        self.deconv3_1_6 = nn.ConvTranspose2d(1,3, kernel_size=3, stride=4, padding=0)\n",
    "        \n",
    "        self.conv5_1_1 = nn.Conv2d(512, 64, kernel_size=3, padding=1)\n",
    "        self.conv5_2_1 = nn.Conv2d(512, 64, kernel_size=3, padding=1)\n",
    "        self.conv5_3_1 = nn.Conv2d(512, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv5_1_3 = nn.Conv2d(64, 1, kernel_size = 3, padding = 1)\n",
    "        self.conv5_1_4 = nn.Conv2d(1, 1, kernel_size=1, padding=0)\n",
    "        self.deconv5_1_5 = nn.ConvTranspose2d(1,3, kernel_size=3, stride=16, padding=0)\n",
    "        \n",
    "        \n",
    "        self.conv_output = nn.Conv2d(1, 1, kernel_size = 1, padding = 0)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.crop = nn.Upsample(size=(448,448))\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        x_1_1 = F.relu(self.conv1_1(x))\n",
    "        x_1_2 = F.relu(self.conv1_2(x_1_1))\n",
    "        x_1_pool = self.pool(x_1_2)\n",
    "\n",
    "        x_2_1 = F.relu(self.conv2_1(x_1_pool))\n",
    "        x_2_2 = F.relu(self.conv2_2(x_2_1))\n",
    "        x_2_pool = self.pool(x_2_2)\n",
    "\n",
    "        x_3_1 = F.relu(self.conv3_1(x_2_pool))\n",
    "        x_3_2 = F.relu(self.conv3_2(x_3_1))\n",
    "        x_3_3 = F.relu(self.conv3_3(x_3_2))\n",
    "        x_3_pool = self.pool(x_3_3)\n",
    "\n",
    "        x_4_1 = F.relu(self.conv4_1(x_3_pool))\n",
    "        x_4_2 = F.relu(self.conv4_2(x_4_1))\n",
    "        x_4_3 = F.relu(self.conv4_3(x_4_2))\n",
    "        x_4_pool = self.pool(x_4_3)\n",
    "\n",
    "        x_5_1 = F.relu(self.conv5_1(x_4_pool))\n",
    "        x_5_2 = F.relu(self.conv5_2(x_5_1))\n",
    "        x_5_3 = F.relu(self.conv5_3(x_5_2))\n",
    "        x_5_pool = self.pool(x_5_3)\n",
    "\n",
    "        x_3_1_1 = F.relu(self.conv3_1_1(x_3_1))\n",
    "        x_3_2_1 = F.relu(self.conv3_2_1(x_3_2))\n",
    "        x_3_3_1 = F.relu(self.conv3_3_1(x_3_3))\n",
    "\n",
    "        x_3_1_2 = x_3_1_1 + x_3_2_1 + x_3_3_1\n",
    "        \n",
    "        x_3_1_3 = F.relu(self.conv3_1_3(x_3_1_2))\n",
    "        x_3_1_4 = F.relu(self.conv3_1_4(x_3_1_3))\n",
    "        x_3_1_5 = F.relu(self.conv3_1_5(x_3_1_4))\n",
    "        x_3_1_6 = F.relu(self.deconv3_1_6(x_3_1_5))\n",
    "        x_3_1_7 = self.crop(x_3_1_6)\n",
    "        \n",
    "        x_5_1_1 = F.relu(self.conv5_1_1(x_5_1))\n",
    "        x_5_2_1 = F.relu(self.conv5_2_1(x_5_2))\n",
    "        x_5_3_1 = F.relu(self.conv5_3_1(x_5_3))\n",
    "        \n",
    "        x_5_1_2 = x_5_1_1 + x_5_2_1 + x_5_3_1\n",
    "        \n",
    "        x_5_1_3 = F.relu(self.conv5_1_3(x_5_1_2))\n",
    "        x_5_1_4 = F.relu(self.conv5_1_4(x_5_1_3))\n",
    "        x_5_1_5 = F.relu(self.deconv5_1_5(x_5_1_4))\n",
    "        x_5_1_6 = self.crop(x_5_1_5)\n",
    "        \n",
    "        x_output = torch.cat((x_3_1_7, x_5_1_6), dim=1)\n",
    "        x_output = F.softmax(self.conv_output(x_output))\n",
    "        \n",
    "        #x = x.view(-1, 7 * 7 * 512)\n",
    "        #x = F.relu(self.fc6(x))\n",
    "        #x = F.dropout(x, 0.5, training=training)\n",
    "        #x = F.relu(self.fc7(x))\n",
    "        #x = F.dropout(x, 0.5, training=training)\n",
    "        #x = self.fc8(x)\n",
    "        return x_output\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = F.softmax(self.forward(x, training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 1, 1, 1], expected input[2, 6, 448, 448] to have 1 channels, but got 6 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-20b4f3b392a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtraining_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#for i, data in enumerate(training_dataloader,0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#inputs, labels = data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fbbbabcc01b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mx_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3_1_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_5_1_6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mx_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#x = x.view(-1, 7 * 7 * 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 1, 1, 1], expected input[2, 6, 448, 448] to have 1 channels, but got 6 channels instead"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    DATA_DIR = \"/media/preethamam/Utilities-SSD/Xtreme_Programming/Z_Data/DLCrack/Liu+Xincong+DS3+CrackSegNet\"\n",
    "    images_dir = os.path.join(DATA_DIR , \"TrainingCracks\")\n",
    "    masks_dir = os.path.join(DATA_DIR , \"TrainingCracksGroundtruth\")\n",
    "    train_data = Dataset(images_dir=images_dir, masks_dir = masks_dir)\n",
    "    training_dataloader = DataLoader(train_data, batch_size=8, shuffle = True)\n",
    "    model = VGG().to(device)\n",
    "    summary(model,input_size = (3, 448,448))\n",
    "    #for i, data in enumerate(training_dataloader,0):\n",
    "        #inputs, labels = data\n",
    "        #optimizer.zero_grad()\n",
    "        #outputs = model(inputs)\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        #running_loss = loss.item()\n",
    "        #if i % 2000\n",
    "    #print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DATA_DIR = \"/media/preethamam/Utilities-SSD/Xtreme_Programming/Z_Data/DLCrack/Liu+Xincong+DS3+CrackSegNet\"\n",
    "    images_dir = os.path.join(DATA_DIR , \"TrainingCracks\")\n",
    "    masks_dir = os.path.join(DATA_DIR , \"TrainingCracksGroundtruth\")\n",
    "    train_data = Dataset(images_dir=images_dir, masks_dir = masks_dir)\n",
    "    training_dataloader = DataLoader(train_data, batch_size=8, shuffle = True)\n",
    "    model = VGG().to(device)\n",
    "    summary(model,input_size = (3, 448,448))\n",
    "    #for i, data in enumerate(training_dataloader,0):\n",
    "        #inputs, labels = data\n",
    "        #optimizer.zero_grad()\n",
    "        #outputs = model(inputs)\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        #running_loss = loss.item()\n",
    "        #if i % 2000\n",
    "    #print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
